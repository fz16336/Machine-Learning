{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Farrel\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Farrel\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:390: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Farrel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Farrel\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 2s 191us/step - loss: 0.4979 - acc: 0.7957\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.4169 - acc: 0.8140\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.4033 - acc: 0.8249\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3928 - acc: 0.8279\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 1s 117us/step - loss: 0.3847 - acc: 0.8304\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 1s 111us/step - loss: 0.3791 - acc: 0.8304\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 1s 104us/step - loss: 0.3753 - acc: 0.8375\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 1s 96us/step - loss: 0.3722 - acc: 0.8442\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.3691 - acc: 0.8444\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3672 - acc: 0.8495\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3648 - acc: 0.8511\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 1s 93us/step - loss: 0.3630 - acc: 0.8517\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 1s 95us/step - loss: 0.3610 - acc: 0.8519\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3592 - acc: 0.8547\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3581 - acc: 0.8536\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 1s 113us/step - loss: 0.3558 - acc: 0.8551\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.3556 - acc: 0.8549\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 1s 140us/step - loss: 0.3542 - acc: 0.8552\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.3538 - acc: 0.8560\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 0.3525 - acc: 0.8575\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 1s 123us/step - loss: 0.3524 - acc: 0.8552\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3512 - acc: 0.8572\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3511 - acc: 0.8592\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3491 - acc: 0.8582\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.3504 - acc: 0.8550\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3492 - acc: 0.8582\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 1s 159us/step - loss: 0.3491 - acc: 0.8570\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 1s 165us/step - loss: 0.3485 - acc: 0.8602 0s - loss: 0.3483 - acc:\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 1s 153us/step - loss: 0.3480 - acc: 0.8575\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 1s 138us/step - loss: 0.3478 - acc: 0.8581\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 1s 102us/step - loss: 0.3475 - acc: 0.8595\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 1s 127us/step - loss: 0.3467 - acc: 0.8576\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 1s 156us/step - loss: 0.3464 - acc: 0.8586\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 1s 139us/step - loss: 0.3462 - acc: 0.8595\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.3462 - acc: 0.8594\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3459 - acc: 0.8579\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3451 - acc: 0.8581\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3445 - acc: 0.8591\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3449 - acc: 0.8590\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3454 - acc: 0.8597\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3438 - acc: 0.8600\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 1s 121us/step - loss: 0.3437 - acc: 0.8612\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 1s 132us/step - loss: 0.3439 - acc: 0.8601\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 1s 98us/step - loss: 0.3442 - acc: 0.8584\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3432 - acc: 0.8609\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3437 - acc: 0.8600\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3425 - acc: 0.8587\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3428 - acc: 0.8585\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3419 - acc: 0.8589\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3433 - acc: 0.8616\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3422 - acc: 0.8597\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3429 - acc: 0.8614\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3419 - acc: 0.8585\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3412 - acc: 0.8599\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3405 - acc: 0.8606\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3406 - acc: 0.8582\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3400 - acc: 0.8617\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 1s 100us/step - loss: 0.3401 - acc: 0.8596\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3394 - acc: 0.8597\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3392 - acc: 0.8602\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3394 - acc: 0.8631\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3396 - acc: 0.8587\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3388 - acc: 0.8625\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 1s 108us/step - loss: 0.3384 - acc: 0.8625\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3390 - acc: 0.8632\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3387 - acc: 0.8622\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3387 - acc: 0.8606\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 1s 87us/step - loss: 0.3395 - acc: 0.8606\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3380 - acc: 0.8607\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 1s 88us/step - loss: 0.3385 - acc: 0.8597\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3374 - acc: 0.8607\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3381 - acc: 0.8609\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3380 - acc: 0.8620\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3384 - acc: 0.8586\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3378 - acc: 0.8594\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 1s 85us/step - loss: 0.3377 - acc: 0.8616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3382 - acc: 0.8600\n",
      "Epoch 78/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.3378 - acc: 0.8581\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 1s 81us/step - loss: 0.3376 - acc: 0.8631\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3374 - acc: 0.8627\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3376 - acc: 0.8609\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3376 - acc: 0.8605\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3382 - acc: 0.8606\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3377 - acc: 0.8607\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3382 - acc: 0.8609\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 1s 86us/step - loss: 0.3370 - acc: 0.8620\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 1s 97us/step - loss: 0.3380 - acc: 0.8592\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3379 - acc: 0.8630\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3377 - acc: 0.8590\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 1s 82us/step - loss: 0.3364 - acc: 0.8625\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 1s 83us/step - loss: 0.3374 - acc: 0.8604\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3376 - acc: 0.8622\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 1s 92us/step - loss: 0.3369 - acc: 0.8595\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3371 - acc: 0.8609\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 1s 84us/step - loss: 0.3375 - acc: 0.8616\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 1s 90us/step - loss: 0.3369 - acc: 0.8600\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 1s 125us/step - loss: 0.3370 - acc: 0.8610\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 1s 144us/step - loss: 0.3378 - acc: 0.8594\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 1s 115us/step - loss: 0.3358 - acc: 0.8634\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 1s 94us/step - loss: 0.3369 - acc: 0.8590\n"
     ]
    }
   ],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('data/Churn_Modelling.csv')\n",
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values\n",
    "\n",
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
    "onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "X = onehotencoder.fit_transform(X).toarray()\n",
    "X = X[:, 1:]\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improving and Tuning the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, batch_size = 10, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10, n_jobs = -1)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "\n",
    "# Dropout Regularization to reduce overfitting if needed\n",
    "\n",
    "# Tuning the ANN\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier)\n",
    "parameters = {'batch_size': [25, 32],\n",
    "              'epochs': [100, 500],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "grid_search = GridSearchCV(estimator = classifier,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 10)\n",
    "grid_search = grid_search.fit(X_train, y_train)\n",
    "best_parameters = grid_search.best_params_\n",
    "best_accuracy = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))\n",
    "\n",
    "# Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "# Flattening\n",
    "classifier.add(Flatten())\n",
    "\n",
    "# Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary')\n",
    "\n",
    "classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 8000,\n",
    "                         epochs = 25,\n",
    "                         validation_data = test_set,\n",
    "                         validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the training set\n",
    "dataset_train = pd.read_csv('Google_Stock_Price_Train.csv')\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "# Creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "\n",
    "# Building the RNN\n",
    "\n",
    "# Importing the Keras libraries and packages\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# Initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# Adding the first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50, return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding a fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# Adding the output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# Compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# Fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n",
    "\n",
    "# Making the predictions and visualising the results\n",
    "\n",
    "# Getting the real stock price of 2017\n",
    "dataset_test = pd.read_csv('Google_Stock_Price_Test.csv')\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "\n",
    "# Getting the predicted stock price of 2017\n",
    "dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red', label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue', label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoEncoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Importing the dataset\n",
    "movies = pd.read_csv('ml-1m/movies.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "users = pd.read_csv('ml-1m/users.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "ratings = pd.read_csv('ml-1m/ratings.dat', sep = '::', header = None, engine = 'python', encoding = 'latin-1')\n",
    "\n",
    "# Preparing the training set and the test set\n",
    "training_set = pd.read_csv('ml-100k/u1.base', delimiter = '\\t')\n",
    "training_set = np.array(training_set, dtype = 'int')\n",
    "test_set = pd.read_csv('ml-100k/u1.test', delimiter = '\\t')\n",
    "test_set = np.array(test_set, dtype = 'int')\n",
    "\n",
    "# Getting the number of users and movies\n",
    "nb_users = int(max(max(training_set[:,0]), max(test_set[:,0])))\n",
    "nb_movies = int(max(max(training_set[:,1]), max(test_set[:,1])))\n",
    "\n",
    "# Converting the data into an array with users in lines and movies in columns\n",
    "def convert(data):\n",
    "    new_data = []\n",
    "    for id_users in range(1, nb_users + 1):\n",
    "        id_movies = data[:,1][data[:,0] == id_users]\n",
    "        id_ratings = data[:,2][data[:,0] == id_users]\n",
    "        ratings = np.zeros(nb_movies)\n",
    "        ratings[id_movies - 1] = id_ratings\n",
    "        new_data.append(list(ratings))\n",
    "    return new_data\n",
    "training_set = convert(training_set)\n",
    "test_set = convert(test_set)\n",
    "\n",
    "# Converting the data into Torch tensors\n",
    "training_set = torch.FloatTensor(training_set)\n",
    "test_set = torch.FloatTensor(test_set)\n",
    "\n",
    "# Creating the architecture of the Neural Network\n",
    "class SAE(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super(SAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(nb_movies, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        self.fc3 = nn.Linear(10, 20)\n",
    "        self.fc4 = nn.Linear(20, nb_movies)\n",
    "        self.activation = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.activation(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "sae = SAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.RMSprop(sae.parameters(), lr = 0.01, weight_decay = 0.5)\n",
    "\n",
    "# Training the SAE\n",
    "nb_epoch = 200\n",
    "for epoch in range(1, nb_epoch + 1):\n",
    "    train_loss = 0\n",
    "    s = 0.\n",
    "    for id_user in range(nb_users):\n",
    "        input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "        target = input.clone()\n",
    "        if torch.sum(target.data > 0) > 0:\n",
    "            output = sae(input)\n",
    "            target.require_grad = False\n",
    "            output[target == 0] = 0\n",
    "            loss = criterion(output, target)\n",
    "            mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "            loss.backward()\n",
    "            train_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "            s += 1.\n",
    "            optimizer.step()\n",
    "    print('epoch: '+str(epoch)+' loss: '+str(train_loss/s))\n",
    "\n",
    "# Testing the SAE\n",
    "test_loss = 0\n",
    "s = 0.\n",
    "for id_user in range(nb_users):\n",
    "    input = Variable(training_set[id_user]).unsqueeze(0)\n",
    "    target = Variable(test_set[id_user])\n",
    "    if torch.sum(target.data > 0) > 0:\n",
    "        output = sae(input)\n",
    "        target.require_grad = False\n",
    "        output[target == 0] = 0\n",
    "        loss = criterion(output, target)\n",
    "        mean_corrector = nb_movies/float(torch.sum(target.data > 0) + 1e-10)\n",
    "        test_loss += np.sqrt(loss.data[0]*mean_corrector)\n",
    "        s += 1.\n",
    "print('test loss: '+str(test_loss/s))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
