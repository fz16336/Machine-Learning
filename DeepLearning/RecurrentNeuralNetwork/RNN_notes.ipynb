{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition\n",
    "A type of neural network designed to recognise patterns in sequential data by allowing information to be stored in 'memory.' Because it posseses internal memory; capable of remembering its past inputs, it is the prefered model for sequential data because it has internal memory, e.g area of application, *time-series, speech, text, weather audio and video* data.\n",
    "\n",
    "* **When do you need it RNN?** \n",
    "    *“Whenever there is a sequence of data and that temporal dynamics that connects the data is more important than the spatial content of each individual frame.”- Lex Fridman (MIT)*\n",
    "\n",
    "### How they work\n",
    "* #### Feed-forward neural network: \n",
    "    * Information only moves forward; information never touches a node twice.\n",
    "    * Have no memory of the input they received previously and are therefore  \n",
    "      bad in predicting what’s coming next.\n",
    "    * Can’t remember anything about what happened in the past, except their \n",
    "      training.\n",
    "    * Map one input to   \n",
    "* #### Recurrent Neural Network\n",
    "    * Information **cycles through a loop**. When it makes a decision, it takes \n",
    "      into consideration the current input and also what it has learned from \n",
    "      the inputs it received previously.\n",
    "    * Has short-term memory, but with **LSTM** they also have long-term \n",
    "      memory.\n",
    "    * Has two inputs, present and past: apply weights to the current and also \n",
    "      to the previous input.\n",
    "    * Can map one to many, many to one (clasifying a voice), many to many\n",
    "    * You can view a RNN as a sequence of Neural Networks that you train one after another with backpropagation.\n",
    "    \n",
    "![RNN](../_images/RNN.png)\n",
    "\n",
    "### Backpropagation Through Time\n",
    "* **Backpropagation**\n",
    "Going backwards through your the network to find the partial derivatives of the error with respect to the weights; enabling you to subtract this value from the weights. Those derivatives are then used by Gradient Descent, an to iteratively minimize a given loss function. Then it adjusts the weights up or down, depending on which decreases the error. That is exactly \n",
    "\n",
    "* **Backpropagation Through Time (BPTT)**\n",
    "Backpropagation on an unrolled Recurrent Neural Network. Unrolling/unfolding is a visualization and conceptual tool, which helps you to understand what’s going on within the network. Since the error of a given timestep depends on the previous timestep, the error is back-propagated from the last to the first timestep, while unrolling all the timesteps. This allows calculating the error for each timestep, which allows updating the weights. Note that BPTT can be computationally expensive when you have a high number of timesteps\n",
    "\n",
    "### Common issues:\n",
    "* **Exploding gradients**\n",
    "when the algorithm assigns a stupidly high importance to the weights, but can be solve if you truncate/squash the gradients\n",
    "\n",
    "* **Vanishing gradients**\n",
    "when the values of a gradient are too small causing the model to stop learning or takes way too long to converge. LSTM is a workaround this problem.\n",
    "\n",
    "![unfold](../_images/unfold.png)\n",
    "\n",
    "### Long short term memory\n",
    "* An extension for recurrent neural networks, which basically extends their memory.\n",
    "* Well suited to learn from important experiences that have very long time lags in between.\n",
    "* Enables the RNN to remember their inputs over a long period of time.\n",
    "* Contain their information in a memory, that is much like the memory of a computer because the LSTM can read, write and delete information from its memory.\n",
    "* Have three analog gates: \n",
    "    * **input gate**: determine whether or not to let new input in\n",
    "    * **forget gate**: determine wheteher or not to delete the irrelvant information\n",
    "    * **output gate**: determine whether or not to impact the output at the current time step\n",
    "* The gates are analog in the form of sigmoids (range from 0 to 1)\n",
    "* Vanishing gradients is solved through LSTM because it keeps the gradients steep enough and therefore the training relatively short and the accuracy high.\n",
    "\n",
    "![LSTM](../_images/LSTM.png)\n",
    "\n",
    "Read more: [skymind.ai](https://skymind.ai/wiki/lstm), [medium.com](https://medium.com/explore-artificial-intelligence/an-introduction-to-recurrent-neural-networks-72c97bf0912), [wildML](www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/), [colah.github.io]("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = pd.read_csv('../_data/Google_Stock_Price_Train.csv')\n",
    "training_set = dataset_train.iloc[:, 1:2].values\n",
    "\n",
    "# scale features\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler(feature_range = (0, 1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "\n",
    "# creating a data structure with 60 timesteps and 1 output\n",
    "X_train = []\n",
    "y_train = []\n",
    "for i in range(60, 1258):\n",
    "    X_train.append(training_set_scaled[i-60:i, 0])\n",
    "    y_train.append(training_set_scaled[i, 0])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "# reshaping\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "print('Dataset dimension: ', dataset_train.shape)\n",
    "dataset_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialising the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# first LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50,\n",
    "                return_sequences = True,\n",
    "                input_shape = (X_train.shape[1], 1)))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# second LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50,\n",
    "                return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# third LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50,\n",
    "                return_sequences = True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# fourth LSTM layer and some Dropout regularisation\n",
    "regressor.add(LSTM(units = 50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# output layer\n",
    "regressor.add(Dense(units = 1))\n",
    "\n",
    "# compiling the RNN\n",
    "regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "# fitting the RNN to the Training set\n",
    "regressor.fit(X_train, y_train, epochs = 100, batch_size = 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result & Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the real stock price of 2017\n",
    "dataset_test = pd.read_csv('../_data/Google_Stock_Price_Test.csv')\n",
    "real_stock_price = dataset_test.iloc[:, 1:2].values\n",
    "\n",
    "# comparing the predicted stock price of 2017\n",
    "dataset_total = pd.concat((dataset_train['Open'],dataset_test['Open']),axis = 0)\n",
    "inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values\n",
    "inputs = inputs.reshape(-1,1)\n",
    "inputs = sc.transform(inputs)\n",
    "\n",
    "X_test = []\n",
    "for i in range(60, 80):\n",
    "    X_test.append(inputs[i-60:i, 0])\n",
    "\n",
    "X_test = np.array(X_test)\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
    "predicted_stock_price = regressor.predict(X_test)\n",
    "predicted_stock_price = sc.inverse_transform(predicted_stock_price)\n",
    "\n",
    "# Visualising the results\n",
    "plt.plot(real_stock_price, color = 'red',\n",
    "        label = 'Real Google Stock Price')\n",
    "plt.plot(predicted_stock_price, color = 'blue',\n",
    "        label = 'Predicted Google Stock Price')\n",
    "plt.title('Google Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Google Stock Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
